{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "from keras.callbacks import TensorBoard\n",
    "#from sklearn.externals import joblib\n",
    "import datetime\n",
    "from keras.models import model_from_json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.models import Sequential\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Data and stitching together, to make preprocessing easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Test.csv\", usecols=['Topic','Sentiment', 'TweetText']).append(pd.read_csv(\"Train.csv\", usecols=['Topic','Sentiment', 'TweetText']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "apple        1079\n",
       "twitter       953\n",
       "google        867\n",
       "microsoft     856\n",
       "Name: Topic, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Topic.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "764     0\n",
      "2392    2\n",
      "3119    3\n",
      "172     0\n",
      "878     1\n",
      "2753    3\n",
      "2004    2\n",
      "498     0\n",
      "2195    2\n",
      "2841    3\n",
      "Name: LABEL, dtype: int64\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n [1. 0. 0. 0.] a\\n [0. 1. 0. 0.] t\\n [0. 0. 1. 0.] g\\n [0. 0. 0. 1.] m\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_categories = 850\n",
    "shuffled = data.reindex(np.random.permutation(data.index))\n",
    "a = shuffled[shuffled['Topic'] == 'apple'][:num_of_categories]\n",
    "t = shuffled[shuffled['Topic'] == 'twitter'][:num_of_categories]\n",
    "g = shuffled[shuffled['Topic'] == 'google'][:num_of_categories]\n",
    "m = shuffled[shuffled['Topic'] == 'microsoft'][:num_of_categories]\n",
    "concated = pd.concat([a,t,g,m], ignore_index=True)\n",
    "#Shuffle the dataset\n",
    "concated = shuffle(concated)\n",
    "concated['LABEL'] = 0\n",
    "concated.loc[concated['Topic'] == 'apple', 'LABEL'] = 0\n",
    "concated.loc[concated['Topic'] == 'twitter', 'LABEL'] = 1\n",
    "concated.loc[concated['Topic'] == 'google', 'LABEL'] = 2\n",
    "concated.loc[concated['Topic'] == 'microsoft', 'LABEL'] = 3\n",
    "print(concated['LABEL'][:10])\n",
    "labels = to_categorical(concated['LABEL'], num_classes=4)\n",
    "print(labels[:10])\n",
    "if 'Topic' in concated.keys():\n",
    "    concated.drop(['Topic'], axis=1)\n",
    "'''\n",
    " [1. 0. 0. 0.] a\n",
    " [0. 1. 0. 0.] t\n",
    " [0. 0. 1. 0.] g\n",
    " [0. 0. 0. 1.] m\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral       1631\n",
       "irrelevant    1246\n",
       "negative       475\n",
       "positive       403\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "885     2\n",
      "1477    3\n",
      "1270    3\n",
      "832     2\n",
      "1135    2\n",
      "275     0\n",
      "1595    3\n",
      "104     0\n",
      "1361    3\n",
      "982     2\n",
      "Name: LABEL, dtype: int64\n",
      "[[0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n [1. 0. 0. 0.] n\\n [0. 1. 0. 0.] i\\n [0. 0. 1. 0.] neg\\n [0. 0. 0. 1.] pos\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_categories2 = 400\n",
    "shuffled2 = data.reindex(np.random.permutation(data.index))\n",
    "n = shuffled2[shuffled2['Sentiment'] == 'neutral'][:num_of_categories2]\n",
    "i = shuffled2[shuffled2['Sentiment'] == 'irrelevant'][:num_of_categories2]\n",
    "neg = shuffled2[shuffled2['Sentiment'] == 'negative'][:num_of_categories2]\n",
    "pos = shuffled2[shuffled2['Sentiment'] == 'positive'][:num_of_categories2]\n",
    "concated2 = pd.concat([n,i,neg,pos], ignore_index=True)\n",
    "#Shuffle the dataset\n",
    "concated2 = shuffle(concated2)\n",
    "concated2['LABEL'] = 0\n",
    "concated2.loc[concated2['Sentiment'] == 'neutral', 'LABEL'] = 0\n",
    "concated2.loc[concated2['Sentiment'] == 'irrelevant', 'LABEL'] = 1\n",
    "concated2.loc[concated2['Sentiment'] == 'negative', 'LABEL'] = 2\n",
    "concated2.loc[concated2['Sentiment'] == 'positive', 'LABEL'] = 3\n",
    "print(concated2['LABEL'][:10])\n",
    "labels2 = to_categorical(concated2['LABEL'], num_classes=4)\n",
    "print(labels2[:10])\n",
    "if 'Sentiment' in concated2.keys():\n",
    "    concated2.drop(['Sentiment'], axis=1)\n",
    "'''\n",
    " [1. 0. 0. 0.] n\n",
    " [0. 1. 0. 0.] i\n",
    " [0. 0. 1. 0.] neg\n",
    " [0. 0. 0. 1.] pos\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9338 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "n_most_common_words = 500\n",
    "max_len = 130\n",
    "tokenizer = Tokenizer(num_words=n_most_common_words, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(concated['TweetText'].values)\n",
    "sequences = tokenizer.texts_to_sequences(concated['TweetText'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "X = pad_sequences(sequences, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X , labels, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(850, 130) (2550, 130)\n",
      "(850, 4) (2550, 4)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape, X_train.shape)\n",
    "print(y_test.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5452 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer2 = Tokenizer(num_words=n_most_common_words, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer2.fit_on_texts(concated2['TweetText'].values)\n",
    "sequences2 = tokenizer2.texts_to_sequences(concated2['TweetText'].values)\n",
    "word_index2 = tokenizer2.word_index\n",
    "print('Found %s unique tokens.' % len(word_index2))\n",
    "\n",
    "X2 = pad_sequences(sequences2, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2 , labels2, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 130) (1200, 130)\n",
      "(400, 4) (1200, 4)\n"
     ]
    }
   ],
   "source": [
    "print(X_test2.shape, X_train2.shape)\n",
    "print(y_test2.shape, y_train2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(x, y, x_val, y_val, embed_dim = 128, lstm = 64, epochs = 10, batch_size = 256, optimizer='adam', verbose=1, name=\"lstm_\"):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(500, embed_dim, input_length=x.shape[1]))\n",
    "    model.add(SpatialDropout1D(0.7))\n",
    "    model.add(LSTM(lstm, dropout=0.7, recurrent_dropout=0.7))\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "    print(model.summary())\n",
    "    filename = \"ed:{},lstm:{},ep:{},bs:{},opt:{},ts:{}\".format(embed_dim, lstm, epochs, batch_size, optimizer, datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "    history = model.fit(x, y, epochs=epochs, verbose=verbose, callbacks=[TensorBoard(log_dir=\"tb/\" + name + filename, histogram_freq=0, write_graph=False)], \n",
    "                        validation_data=(x_val, y_val), batch_size=batch_size)\n",
    "    with open(\"models/\" + name + \"model_\" + filename + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model.to_json())\n",
    "    model.save_weights(\"models/\" + name + \"model_\" + filename + \".h5\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters used for Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"epochs = [10, 20, 30] #22\\nbatch_size = [150, 250, 500] #150\\n\\nembed_dims = [50, 100, 200]  #standard\\nlstm = [30, 50, 100]  #standard\\noptimizer = ['Nadam', 'Adadelta', 'Adagrad', 'Adam', 'RMSprop']  #adagrad\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"epochs = [10, 20, 30] #22\n",
    "batch_size = [150, 250, 500] #150\n",
    "\n",
    "embed_dims = [50, 100, 200]  #standard\n",
    "lstm = [30, 50, 100]  #standard\n",
    "optimizer = ['Nadam', 'Adadelta', 'Adagrad', 'Adam', 'RMSprop']  #adagrad\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 130, 128)          64000     \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_3 (Spatial (None, 130, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 113,668\n",
      "Trainable params: 113,668\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 2550 samples, validate on 850 samples\n",
      "Epoch 1/22\n",
      "2550/2550 [==============================] - 3s 1ms/step - loss: 1.3332 - acc: 0.3753 - val_loss: 1.2072 - val_acc: 0.5624\n",
      "Epoch 2/22\n",
      "2550/2550 [==============================] - 3s 1ms/step - loss: 1.1559 - acc: 0.5082 - val_loss: 1.0040 - val_acc: 0.5918\n",
      "Epoch 3/22\n",
      "2550/2550 [==============================] - 3s 1ms/step - loss: 1.0130 - acc: 0.5886 - val_loss: 0.8894 - val_acc: 0.6894\n",
      "Epoch 4/22\n",
      "2550/2550 [==============================] - 3s 1ms/step - loss: 0.9271 - acc: 0.6400 - val_loss: 0.8132 - val_acc: 0.7153\n",
      "Epoch 5/22\n",
      "2550/2550 [==============================] - 3s 1ms/step - loss: 0.8613 - acc: 0.6686 - val_loss: 0.7649 - val_acc: 0.7176\n",
      "Epoch 6/22\n",
      "2550/2550 [==============================] - 3s 1ms/step - loss: 0.8183 - acc: 0.6729 - val_loss: 0.7262 - val_acc: 0.7271\n",
      "Epoch 7/22\n",
      "2550/2550 [==============================] - 3s 1ms/step - loss: 0.7998 - acc: 0.6984 - val_loss: 0.6961 - val_acc: 0.7329\n",
      "Epoch 8/22\n",
      "2550/2550 [==============================] - 3s 1ms/step - loss: 0.7439 - acc: 0.7118 - val_loss: 0.6609 - val_acc: 0.7565\n",
      "Epoch 9/22\n",
      "2550/2550 [==============================] - 3s 1ms/step - loss: 0.7043 - acc: 0.7204 - val_loss: 0.6399 - val_acc: 0.7541\n",
      "Epoch 10/22\n",
      "2550/2550 [==============================] - 3s 1ms/step - loss: 0.6846 - acc: 0.7529 - val_loss: 0.6185 - val_acc: 0.7588\n",
      "Epoch 11/22\n",
      "2550/2550 [==============================] - 3s 1ms/step - loss: 0.6587 - acc: 0.7494 - val_loss: 0.6047 - val_acc: 0.7576\n",
      "Epoch 12/22\n",
      "2550/2550 [==============================] - 3s 1ms/step - loss: 0.6474 - acc: 0.7514 - val_loss: 0.5871 - val_acc: 0.7647\n",
      "Epoch 13/22\n",
      "2550/2550 [==============================] - 3s 1ms/step - loss: 0.6298 - acc: 0.7600 - val_loss: 0.5743 - val_acc: 0.7718\n",
      "Epoch 14/22\n",
      "2550/2550 [==============================] - 3s 1ms/step - loss: 0.6187 - acc: 0.7588 - val_loss: 0.5626 - val_acc: 0.7694\n",
      "Epoch 15/22\n",
      "2550/2550 [==============================] - 3s 1ms/step - loss: 0.5984 - acc: 0.7757 - val_loss: 0.5544 - val_acc: 0.7741\n",
      "Epoch 16/22\n",
      "2550/2550 [==============================] - 3s 1ms/step - loss: 0.5779 - acc: 0.7776 - val_loss: 0.5433 - val_acc: 0.7835\n",
      "Epoch 17/22\n",
      "2550/2550 [==============================] - 3s 1ms/step - loss: 0.5788 - acc: 0.7843 - val_loss: 0.5376 - val_acc: 0.7824\n",
      "Epoch 18/22\n",
      "2550/2550 [==============================] - 3s 1ms/step - loss: 0.5593 - acc: 0.7914 - val_loss: 0.5383 - val_acc: 0.7824\n",
      "Epoch 19/22\n",
      "2550/2550 [==============================] - 3s 1ms/step - loss: 0.5562 - acc: 0.7831 - val_loss: 0.5290 - val_acc: 0.7906\n",
      "Epoch 20/22\n",
      "2550/2550 [==============================] - 3s 1ms/step - loss: 0.5560 - acc: 0.7910 - val_loss: 0.5308 - val_acc: 0.7929\n",
      "Epoch 21/22\n",
      "2550/2550 [==============================] - 3s 1ms/step - loss: 0.5299 - acc: 0.8043 - val_loss: 0.5235 - val_acc: 0.7976\n",
      "Epoch 22/22\n",
      "2550/2550 [==============================] - 3s 1ms/step - loss: 0.5310 - acc: 0.7929 - val_loss: 0.5208 - val_acc: 0.7953\n"
     ]
    }
   ],
   "source": [
    "cat_model = create_model(X_train, y_train, X_test, y_test, epochs=22, batch_size=150, optimizer='Adagrad', name=\"cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 130, 128)          64000     \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_4 (Spatial (None, 130, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 113,668\n",
      "Trainable params: 113,668\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1200 samples, validate on 400 samples\n",
      "Epoch 1/22\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 1.3759 - acc: 0.3075 - val_loss: 1.3442 - val_acc: 0.4525\n",
      "Epoch 2/22\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 1.3271 - acc: 0.3858 - val_loss: 1.2957 - val_acc: 0.4650\n",
      "Epoch 3/22\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 1.2777 - acc: 0.4458 - val_loss: 1.2323 - val_acc: 0.4800\n",
      "Epoch 4/22\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 1.2199 - acc: 0.4800 - val_loss: 1.1808 - val_acc: 0.5025\n",
      "Epoch 5/22\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 1.1669 - acc: 0.4833 - val_loss: 1.1533 - val_acc: 0.5050\n",
      "Epoch 6/22\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 1.1344 - acc: 0.5117 - val_loss: 1.1275 - val_acc: 0.5000\n",
      "Epoch 7/22\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 1.1005 - acc: 0.5308 - val_loss: 1.1071 - val_acc: 0.5050\n",
      "Epoch 8/22\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 1.0801 - acc: 0.5383 - val_loss: 1.0934 - val_acc: 0.5125\n",
      "Epoch 9/22\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 1.0669 - acc: 0.5650 - val_loss: 1.0836 - val_acc: 0.5300\n",
      "Epoch 10/22\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 1.0398 - acc: 0.5825 - val_loss: 1.0742 - val_acc: 0.5300\n",
      "Epoch 11/22\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 1.0163 - acc: 0.5867 - val_loss: 1.0642 - val_acc: 0.5475\n",
      "Epoch 12/22\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.9972 - acc: 0.5792 - val_loss: 1.0578 - val_acc: 0.5450\n",
      "Epoch 13/22\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.9843 - acc: 0.6158 - val_loss: 1.0536 - val_acc: 0.5550\n",
      "Epoch 14/22\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.9784 - acc: 0.6042 - val_loss: 1.0446 - val_acc: 0.5550\n",
      "Epoch 15/22\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.9498 - acc: 0.6217 - val_loss: 1.0311 - val_acc: 0.5625\n",
      "Epoch 16/22\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.9457 - acc: 0.6225 - val_loss: 1.0327 - val_acc: 0.5600\n",
      "Epoch 17/22\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.9339 - acc: 0.6258 - val_loss: 1.0270 - val_acc: 0.5600\n",
      "Epoch 18/22\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.9284 - acc: 0.6192 - val_loss: 1.0206 - val_acc: 0.5675\n",
      "Epoch 19/22\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.9215 - acc: 0.6283 - val_loss: 1.0167 - val_acc: 0.5725\n",
      "Epoch 20/22\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.9009 - acc: 0.6475 - val_loss: 1.0076 - val_acc: 0.5800\n",
      "Epoch 21/22\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8797 - acc: 0.6525 - val_loss: 1.0096 - val_acc: 0.5750\n",
      "Epoch 22/22\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8768 - acc: 0.6608 - val_loss: 1.0008 - val_acc: 0.5875\n"
     ]
    }
   ],
   "source": [
    "sent_model = create_model(X_train2, y_train2, X_test2, y_test2, epochs=22, batch_size=150, optimizer='Adagrad', name=\"sent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "850/850 [==============================] - 1s 1ms/step\n",
      "Test set\n",
      "  Loss: 0.521\n",
      "  Accuracy: 0.795\n"
     ]
    }
   ],
   "source": [
    "accr = cat_model.evaluate(X_test,y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 0s 1ms/step\n",
      "Test set\n",
      "  Loss: 1.001\n",
      "  Accuracy: 0.588\n"
     ]
    }
   ],
   "source": [
    "accr2 = sent_model.evaluate(X_test2,y_test2)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr2[0],accr2[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.38740495 0.13121562 0.2631111  0.21826828]] apple\n",
      "[[0.13929802 0.00575931 0.33990443 0.5150383 ]] positive\n"
     ]
    }
   ],
   "source": [
    "txt = [\"Getting a lot of spam at my iphone thats nice\"]\n",
    "seq = tokenizer.texts_to_sequences(txt)\n",
    "padded = pad_sequences(seq, maxlen=max_len)\n",
    "pred = cat_model.predict(padded)\n",
    "labels = ['apple', 'twitter', 'google', 'microsoft']\n",
    "print(pred, labels[np.argmax(pred)])\n",
    "seq = tokenizer2.texts_to_sequences(txt)\n",
    "padded = pad_sequences(seq, maxlen=max_len)\n",
    "pred = sent_model.predict(padded)\n",
    "labels = ['neutral', 'irrelevant', 'negative', 'positive']\n",
    "print(pred, labels[np.argmax(pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/sent_tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer2, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env3",
   "language": "python",
   "name": "env3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
