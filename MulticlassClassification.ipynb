{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "from keras.callbacks import TensorBoard\n",
    "from sklearn.externals import joblib\n",
    "import datetime\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.models import Sequential\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Test.csv\", usecols=['Topic','Sentiment', 'TweetText']).append(pd.read_csv(\"Train.csv\", usecols=['Topic','Sentiment', 'TweetText']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "apple        1079\n",
       "twitter       953\n",
       "google        867\n",
       "microsoft     856\n",
       "Name: Topic, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Topic.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2244    2\n",
      "826     0\n",
      "1416    1\n",
      "1088    1\n",
      "3376    3\n",
      "244     0\n",
      "1451    1\n",
      "2242    2\n",
      "3367    3\n",
      "3216    3\n",
      "Name: LABEL, dtype: int64\n",
      "[[0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n [1. 0. 0. 0.] a\\n [0. 1. 0. 0.] t\\n [0. 0. 1. 0.] g\\n [0. 0. 0. 1.] m\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_categories = 850\n",
    "shuffled = data.reindex(np.random.permutation(data.index))\n",
    "a = shuffled[shuffled['Topic'] == 'apple'][:num_of_categories]\n",
    "t = shuffled[shuffled['Topic'] == 'twitter'][:num_of_categories]\n",
    "g = shuffled[shuffled['Topic'] == 'google'][:num_of_categories]\n",
    "m = shuffled[shuffled['Topic'] == 'microsoft'][:num_of_categories]\n",
    "concated = pd.concat([a,t,g,m], ignore_index=True)\n",
    "#Shuffle the dataset\n",
    "concated = shuffle(concated)\n",
    "concated['LABEL'] = 0\n",
    "concated.loc[concated['Topic'] == 'apple', 'LABEL'] = 0\n",
    "concated.loc[concated['Topic'] == 'twitter', 'LABEL'] = 1\n",
    "concated.loc[concated['Topic'] == 'google', 'LABEL'] = 2\n",
    "concated.loc[concated['Topic'] == 'microsoft', 'LABEL'] = 3\n",
    "print(concated['LABEL'][:10])\n",
    "labels = to_categorical(concated['LABEL'], num_classes=4)\n",
    "print(labels[:10])\n",
    "if 'Topic' in concated.keys():\n",
    "    concated.drop(['Topic'], axis=1)\n",
    "'''\n",
    " [1. 0. 0. 0.] a\n",
    " [0. 1. 0. 0.] t\n",
    " [0. 0. 1. 0.] g\n",
    " [0. 0. 0. 1.] m\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral       1631\n",
       "irrelevant    1246\n",
       "negative       475\n",
       "positive       403\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420     1\n",
      "237     0\n",
      "47      0\n",
      "1142    2\n",
      "398     0\n",
      "663     1\n",
      "158     0\n",
      "89      0\n",
      "662     1\n",
      "379     0\n",
      "Name: LABEL, dtype: int64\n",
      "[[0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n [1. 0. 0. 0.] n\\n [0. 1. 0. 0.] i\\n [0. 0. 1. 0.] neg\\n [0. 0. 0. 1.] pos\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_categories2 = 400\n",
    "shuffled2 = data.reindex(np.random.permutation(data.index))\n",
    "n = shuffled2[shuffled2['Sentiment'] == 'neutral'][:num_of_categories2]\n",
    "i = shuffled2[shuffled2['Sentiment'] == 'irrelevant'][:num_of_categories2]\n",
    "neg = shuffled2[shuffled2['Sentiment'] == 'negative'][:num_of_categories2]\n",
    "pos = shuffled2[shuffled2['Sentiment'] == 'positive'][:num_of_categories2]\n",
    "concated2 = pd.concat([n,i,neg,pos], ignore_index=True)\n",
    "#Shuffle the dataset\n",
    "concated2 = shuffle(concated2)\n",
    "concated2['LABEL'] = 0\n",
    "concated2.loc[concated2['Sentiment'] == 'neutral', 'LABEL'] = 0\n",
    "concated2.loc[concated2['Sentiment'] == 'irrelevant', 'LABEL'] = 1\n",
    "concated2.loc[concated2['Sentiment'] == 'negative', 'LABEL'] = 2\n",
    "concated2.loc[concated2['Sentiment'] == 'positive', 'LABEL'] = 3\n",
    "print(concated2['LABEL'][:10])\n",
    "labels2 = to_categorical(concated2['LABEL'], num_classes=4)\n",
    "print(labels2[:10])\n",
    "if 'Sentiment' in concated2.keys():\n",
    "    concated2.drop(['Sentiment'], axis=1)\n",
    "'''\n",
    " [1. 0. 0. 0.] n\n",
    " [0. 1. 0. 0.] i\n",
    " [0. 0. 1. 0.] neg\n",
    " [0. 0. 0. 1.] pos\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9338 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "n_most_common_words = 500\n",
    "max_len = 130\n",
    "tokenizer = Tokenizer(num_words=n_most_common_words, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(concated['TweetText'].values)\n",
    "sequences = tokenizer.texts_to_sequences(concated['TweetText'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "X = pad_sequences(sequences, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X , labels, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(850, 130) (2550, 130)\n",
      "(850, 4) (2550, 4)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape, X_train.shape)\n",
    "print(y_test.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5452 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer2 = Tokenizer(num_words=n_most_common_words, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer2.fit_on_texts(concated2['TweetText'].values)\n",
    "sequences2 = tokenizer2.texts_to_sequences(concated2['TweetText'].values)\n",
    "word_index2 = tokenizer2.word_index\n",
    "print('Found %s unique tokens.' % len(word_index2))\n",
    "\n",
    "X2 = pad_sequences(sequences2, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2 , labels2, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 130) (1200, 130)\n",
      "(400, 4) (1200, 4)\n"
     ]
    }
   ],
   "source": [
    "print(X_test2.shape, X_train2.shape)\n",
    "print(y_test2.shape, y_train2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(x, y, x_val, y_val, embed_dim = 128, lstm = 64, epochs = 10, batch_size = 256, optimizer='adam', verbose=1):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(500, embed_dim, input_length=x.shape[1]))\n",
    "    model.add(SpatialDropout1D(0.7))\n",
    "    model.add(LSTM(lstm, dropout=0.7, recurrent_dropout=0.7))\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "    print(model.summary())\n",
    "    filename = \"ed:{},lstm:{},ep:{},bs:{},opt:{},ts:{}\".format(embed_dim, lstm, epochs, batch_size, optimizer, datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "    history = model.fit(x, y, epochs=epochs, verbose=verbose, callbacks=[TensorBoard(log_dir='tb/lstm2_' + filename, histogram_freq=0, write_graph=False)], \n",
    "                        validation_data=(x_val, y_val), batch_size=batch_size)\n",
    "    #joblib.dump(model, \"models/lstm_\" + filename + \".sav\")\n",
    "    #joblib.dump(history, \"models/h_\" + filename + \".txt\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"epochs = [10, 20, 30] #22\\nbatch_size = [150, 250, 500] #150\\n\\nembed_dims = [50, 100, 200]  #standard\\nlstm = [30, 50, 100]  #standard\\noptimizer = ['Nadam', 'Adadelta', 'Adagrad', 'Adam', 'RMSprop']  #adagrad\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"epochs = [10, 20, 30] #22\n",
    "batch_size = [150, 250, 500] #150\n",
    "\n",
    "embed_dims = [50, 100, 200]  #standard\n",
    "lstm = [30, 50, 100]  #standard\n",
    "optimizer = ['Nadam', 'Adadelta', 'Adagrad', 'Adam', 'RMSprop']  #adagrad\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 130, 128)          64000     \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 130, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 113,668\n",
      "Trainable params: 113,668\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 2550 samples, validate on 850 samples\n",
      "Epoch 1/22\n",
      "2550/2550 [==============================] - 3s 1ms/step - loss: 1.3160 - acc: 0.4165 - val_loss: 1.1825 - val_acc: 0.5565\n",
      "Epoch 2/22\n",
      "2550/2550 [==============================] - 3s 1ms/step - loss: 1.1479 - acc: 0.5224 - val_loss: 0.9839 - val_acc: 0.6541\n",
      "Epoch 3/22\n",
      "2550/2550 [==============================] - 3s 1ms/step - loss: 1.0024 - acc: 0.6004 - val_loss: 0.8624 - val_acc: 0.7165\n",
      "Epoch 4/22\n",
      "2550/2550 [==============================] - 3s 1ms/step - loss: 0.9054 - acc: 0.6431 - val_loss: 0.7829 - val_acc: 0.7306\n",
      "Epoch 5/22\n",
      "2550/2550 [==============================] - 3s 1ms/step - loss: 0.8449 - acc: 0.6702 - val_loss: 0.7296 - val_acc: 0.7412\n",
      "Epoch 6/22\n",
      "2550/2550 [==============================] - 3s 1ms/step - loss: 0.7885 - acc: 0.6929 - val_loss: 0.6947 - val_acc: 0.7482\n",
      "Epoch 7/22\n",
      "2550/2550 [==============================] - 3s 1ms/step - loss: 0.7453 - acc: 0.7157 - val_loss: 0.6642 - val_acc: 0.7553\n",
      "Epoch 8/22\n",
      "2550/2550 [==============================] - 3s 1ms/step - loss: 0.6950 - acc: 0.7365 - val_loss: 0.6411 - val_acc: 0.7529\n",
      "Epoch 9/22\n",
      "2550/2550 [==============================] - 3s 1ms/step - loss: 0.6763 - acc: 0.7361 - val_loss: 0.6278 - val_acc: 0.7576\n",
      "Epoch 10/22\n",
      "2550/2550 [==============================] - 3s 1ms/step - loss: 0.6493 - acc: 0.7592 - val_loss: 0.6098 - val_acc: 0.7694\n",
      "Epoch 11/22\n",
      "2550/2550 [==============================] - 3s 1ms/step - loss: 0.6321 - acc: 0.7482 - val_loss: 0.6012 - val_acc: 0.7718\n",
      "Epoch 12/22\n",
      "2550/2550 [==============================] - 3s 1ms/step - loss: 0.6193 - acc: 0.7694 - val_loss: 0.5876 - val_acc: 0.7718\n",
      "Epoch 13/22\n",
      "2550/2550 [==============================] - 3s 1ms/step - loss: 0.5824 - acc: 0.7686 - val_loss: 0.5800 - val_acc: 0.7729\n",
      "Epoch 14/22\n",
      "2550/2550 [==============================] - 3s 1ms/step - loss: 0.5887 - acc: 0.7702 - val_loss: 0.5759 - val_acc: 0.7776\n",
      "Epoch 15/22\n",
      "2550/2550 [==============================] - 3s 1ms/step - loss: 0.5778 - acc: 0.7753 - val_loss: 0.5681 - val_acc: 0.7835\n",
      "Epoch 16/22\n",
      "2550/2550 [==============================] - 3s 1ms/step - loss: 0.5547 - acc: 0.7882 - val_loss: 0.5601 - val_acc: 0.7835\n",
      "Epoch 17/22\n",
      "2550/2550 [==============================] - 3s 1ms/step - loss: 0.5438 - acc: 0.7886 - val_loss: 0.5571 - val_acc: 0.7871\n",
      "Epoch 18/22\n",
      "2550/2550 [==============================] - 3s 1ms/step - loss: 0.5366 - acc: 0.7925 - val_loss: 0.5568 - val_acc: 0.7882\n",
      "Epoch 19/22\n",
      "2550/2550 [==============================] - 3s 1ms/step - loss: 0.5403 - acc: 0.7961 - val_loss: 0.5544 - val_acc: 0.7929\n",
      "Epoch 20/22\n",
      "2550/2550 [==============================] - 3s 1ms/step - loss: 0.5233 - acc: 0.8016 - val_loss: 0.5497 - val_acc: 0.7894\n",
      "Epoch 21/22\n",
      "2550/2550 [==============================] - 3s 1ms/step - loss: 0.5017 - acc: 0.8106 - val_loss: 0.5454 - val_acc: 0.7882\n",
      "Epoch 22/22\n",
      "2550/2550 [==============================] - 3s 1ms/step - loss: 0.4908 - acc: 0.8125 - val_loss: 0.5424 - val_acc: 0.7906\n"
     ]
    }
   ],
   "source": [
    "cat_model = create_model(X_train, y_train, X_test, y_test, epochs=22, batch_size=150, optimizer='Adagrad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 130, 128)          64000     \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_2 (Spatial (None, 130, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 113,668\n",
      "Trainable params: 113,668\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1200 samples, validate on 400 samples\n",
      "Epoch 1/22\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 1.3761 - acc: 0.3092 - val_loss: 1.3500 - val_acc: 0.4175\n",
      "Epoch 2/22\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 1.3299 - acc: 0.3875 - val_loss: 1.2899 - val_acc: 0.4150\n",
      "Epoch 3/22\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 1.2718 - acc: 0.4392 - val_loss: 1.2249 - val_acc: 0.4375\n",
      "Epoch 4/22\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 1.2066 - acc: 0.4667 - val_loss: 1.1725 - val_acc: 0.4850\n",
      "Epoch 5/22\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 1.1818 - acc: 0.4650 - val_loss: 1.1444 - val_acc: 0.4975\n",
      "Epoch 6/22\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 1.1408 - acc: 0.5017 - val_loss: 1.1198 - val_acc: 0.5000\n",
      "Epoch 7/22\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 1.1232 - acc: 0.5050 - val_loss: 1.0975 - val_acc: 0.5100\n",
      "Epoch 8/22\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 1.1064 - acc: 0.5333 - val_loss: 1.0813 - val_acc: 0.5100\n",
      "Epoch 9/22\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 1.0804 - acc: 0.5408 - val_loss: 1.0645 - val_acc: 0.5350\n",
      "Epoch 10/22\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 1.0486 - acc: 0.5508 - val_loss: 1.0504 - val_acc: 0.5375\n",
      "Epoch 11/22\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 1.0467 - acc: 0.5592 - val_loss: 1.0352 - val_acc: 0.5450\n",
      "Epoch 12/22\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 1.0296 - acc: 0.5842 - val_loss: 1.0267 - val_acc: 0.5450\n",
      "Epoch 13/22\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 1.0082 - acc: 0.5842 - val_loss: 1.0133 - val_acc: 0.5450\n",
      "Epoch 14/22\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.9785 - acc: 0.6067 - val_loss: 1.0005 - val_acc: 0.5450\n",
      "Epoch 15/22\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.9869 - acc: 0.6108 - val_loss: 0.9959 - val_acc: 0.5475\n",
      "Epoch 16/22\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.9708 - acc: 0.6142 - val_loss: 0.9892 - val_acc: 0.5625\n",
      "Epoch 17/22\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.9618 - acc: 0.6133 - val_loss: 0.9779 - val_acc: 0.5575\n",
      "Epoch 18/22\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.9461 - acc: 0.6267 - val_loss: 0.9689 - val_acc: 0.5650\n",
      "Epoch 19/22\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.9132 - acc: 0.6442 - val_loss: 0.9625 - val_acc: 0.5725\n",
      "Epoch 20/22\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.9125 - acc: 0.6267 - val_loss: 0.9501 - val_acc: 0.5800\n",
      "Epoch 21/22\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.9071 - acc: 0.6358 - val_loss: 0.9486 - val_acc: 0.5850\n",
      "Epoch 22/22\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.8938 - acc: 0.6492 - val_loss: 0.9406 - val_acc: 0.5925\n"
     ]
    }
   ],
   "source": [
    "sent_model = create_model(X_train2, y_train2, X_test2, y_test2, epochs=22, batch_size=150, optimizer='Adagrad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "850/850 [==============================] - 1s 1ms/step\n",
      "Test set\n",
      "  Loss: 0.542\n",
      "  Accuracy: 0.791\n"
     ]
    }
   ],
   "source": [
    "accr = cat_model.evaluate(X_test,y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 0s 1ms/step\n",
      "Test set\n",
      "  Loss: 0.941\n",
      "  Accuracy: 0.593\n"
     ]
    }
   ],
   "source": [
    "accr2 = sent_model.evaluate(X_test2,y_test2)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr2[0],accr2[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.61343795 0.07356836 0.15844128 0.15455246]] apple\n",
      "[[0.1922596  0.00790357 0.38403144 0.41580537]] positive\n"
     ]
    }
   ],
   "source": [
    "txt = [\"Getting a lot of spam at my iphone thats nice\"]\n",
    "seq = tokenizer.texts_to_sequences(txt)\n",
    "padded = pad_sequences(seq, maxlen=max_len)\n",
    "pred = cat_model.predict(padded)\n",
    "labels = ['apple', 'twitter', 'google', 'microsoft']\n",
    "print(pred, labels[np.argmax(pred)])\n",
    "seq = tokenizer2.texts_to_sequences(txt)\n",
    "padded = pad_sequences(seq, maxlen=max_len)\n",
    "pred = sent_model.predict(padded)\n",
    "labels = ['neutral', 'irrelevant', 'negative', 'positive']\n",
    "print(pred, labels[np.argmax(pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env3",
   "language": "python",
   "name": "env3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
